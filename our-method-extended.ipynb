{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out methods\n",
    "1) Try raw jammin scores\n",
    "2) Try normalized jammin scores\n",
    "3) Normalized jammin + vader scores \n",
    "4) Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6330822d62a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jammin_emotion(text):\n",
    "    # Pre process text\n",
    "    text = text.replace('\\\\\"','\"')\n",
    "    text = text.replace('\"','\\\\\"')\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace('â€™',\"'\")\n",
    "    text = text.replace('â€œ','\\\\\"')\n",
    "    text = text.replace('â€','\\\\\"')\n",
    "    text = text.replace('â€”','-')\n",
    "    text = text.replace('â€¦','...')\n",
    "    text = text.replace('ðŸ¤”',' ')\n",
    "    text = text.replace('Ÿ™„',' ')\n",
    "    text = text.replace('â€˜',\"'\")\n",
    "    text = text.replace('ðŸ‘',' ')\n",
    "    text = text.replace('ðŸ˜‰',' ')\n",
    "    text = text.replace('ðŸ¤¨',' ')\n",
    "    text = text.replace('ðŸ˜„',' ')\n",
    "    text = text.replace('Ÿ˜‚',' ')\n",
    "    text = text.replace('Ÿ˜',' ')\n",
    "    text = text.replace('€‹',' ')\n",
    "    text = text.replace('€‹',' ')\n",
    "    text = text.replace('„',' ')\n",
    "    text = text.replace('Ÿ',' ')\n",
    "    text = text.replace('€',' ')\n",
    "    text = text.replace('™‚',' ')\n",
    "    text = text.replace('’',' ')\n",
    "    text = text.replace('™',' ')\n",
    "    text = text.replace('‡',' ')\n",
    "    text = text.replace('‘Š',' ')\n",
    "    text = text.replace('–',' ')\n",
    "    text = text.replace('“',' ')\n",
    "    text = text.replace('”',' ')\n",
    "    text = text.replace('š',' ')\n",
    "    text = text.replace('˜',' ')\n",
    "    text = text.replace('œ…',' ')\n",
    "    text = text.replace('œ',' ')\n",
    "    text = text.replace('‹',' ')\n",
    "    text = text.replace('Œ',' ')\n",
    "    text = text.replace('›',' ')\n",
    "    text = text.replace('‘',' ')\n",
    "    text = text.replace('…',' ')\n",
    "    text = text.replace('Ž',' ')\n",
    "    \n",
    "    print(text)\n",
    " \n",
    "    payload = '{\"text\":\"%s\",\"lang\":\"en\"}'%(text)\n",
    "    #print payload\n",
    "    headers = {'content-Type': 'application/json'}\n",
    "\n",
    "    url = 'http://1.34.96.63:8080/webresources/jammin/emotion'\n",
    "    #url = 'http://127.0.0.1:8080/JamminTextEmotionAPI/webresources/jammin/emotion'\n",
    "\n",
    "    r = requests.post(url,  data=payload, headers=headers)\n",
    "    #print(r)\n",
    "    return r.json()\n",
    "\n",
    "def contains_a_d_or_b(response):\n",
    "    if \"text\" in response:\n",
    "        if \"yes\" in response[\"bullying\"]:\n",
    "            #print \"Contains bullying\"\n",
    "            return True\n",
    "        for group in response[\"groups\"]:\n",
    "            if \"anger\" in group[\"name\"]:\n",
    "                #print \"Contains anger\"\n",
    "                return True\n",
    "\n",
    "            if \"disgust\" in group[\"name\"]:\n",
    "                #print \"Contains disgust\"\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_negativity_ind_posts(post,snr,  an, ac):\n",
    "    vector = []\n",
    "\n",
    "    # Count the number of negative sentences\n",
    "    count_neg_sents = 0\n",
    "    neg_ratio = 0.0\n",
    "    count_bullying = 0\n",
    "    bullying_ratio = 0.0\n",
    "\n",
    "    negs = []\n",
    "    compounds = []\n",
    "\n",
    "    # To get the average sentiment scores\n",
    "    tot_sentences = len(post[\"sentences\"])\n",
    "    \n",
    "    # To determine if this comment contains negativity\n",
    "    neg_comment = False\n",
    "    \n",
    "    for sent in post[\"sentences\"]:\n",
    "\n",
    "        # Get the Vader scores\n",
    "        vs = sent[\"vader\"]\n",
    "        neg = float(vs[\"neg\"])\n",
    "        compound = float(vs[\"compound\"])\n",
    "\n",
    "        negs.append(neg)\n",
    "        compounds.append(compound)\n",
    "        # Add the negativity and compound\n",
    "        #sum_neg = sum_neg + neg\n",
    "        #sum_compound = sum_compound + compound\n",
    "\n",
    "        # Get the Jammin scores\n",
    "        je = sent[\"jammin\"]\n",
    "\n",
    "        # If both jammin and vader detect negativity\n",
    "        if contains_a_d_or_b(je) and (neg >= 0.05 or compound <= -0.05):\n",
    "            # Mark this comment as being negative\n",
    "            neg_comment = True\n",
    "\n",
    "            count_neg_sents = count_neg_sents + 1\n",
    "            if \"yes\" in je[\"bullying\"]:\n",
    "                count_bullying = count_bullying + 1\n",
    "\n",
    "    # Flag the post a negative\n",
    "    if neg_comment:\n",
    "        vector.append(1.0)\n",
    "    else:\n",
    "        vector.append(0.0)\n",
    "        \n",
    "\n",
    "    #avg_neg = float(sum_neg) / float(tot_sentences)\n",
    "    #avg_compound = float(sum_compound) / float(tot_sentences)\n",
    "\n",
    "    avg_neg = np.mean(negs)\n",
    "    avg_compound = np.mean(compounds)\n",
    "    \n",
    "    vector.append(avg_neg)\n",
    "    vector.append(avg_compound)\n",
    "\n",
    "    #if tot_sentences > 0:\n",
    "    # Percentage of neg comments for this post\n",
    "    #neg_ratio = float(count_neg) / float(tot_sentences)\n",
    "    # Percentage of neg sentences for this post\n",
    "    sent_neg_ratio = float(count_neg_sents) / float(tot_sentences)\n",
    "    # Percentage of bullying detectec on this post\n",
    "    bullying_ratio = float(count_bullying) / float(tot_sentences)\n",
    "    \n",
    "    vector.append(sent_neg_ratio)\n",
    "    vector.append(bullying_ratio)\n",
    "\n",
    "    if sent_neg_ratio >= snr and (avg_neg >= an or avg_compound <= -ac):\n",
    "        # Determine the level\n",
    "        level = 1\n",
    "\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Excellent reminder of where things could go! California and Sanctuary cities do not speak for the rest of Americans anymore than the KKK and racial laws during the Civil Rights movements.and the illegal laws that accompanied them! Illegal immigrants do not belong here, they are illegal. If immigrants want protection, and then do what you need to become legal and then I will welcome them!\"\n",
    "sentence = ' \" I love how they are so \"concerned\" about gun control.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\\" I love how they are so \\\"concerned\\\" about gun control.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ba685bf4d80d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjammin_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bd02d1eb3e91>\u001b[0m in \u001b[0;36mjammin_emotion\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#url = 'http://127.0.0.1:8080/JamminTextEmotionAPI/webresources/jammin/emotion'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m#print(r)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "print (jammin_emotion(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentimentIntensityAnalyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a2a72272b696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SentimentIntensityAnalyzer' is not defined"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyzer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e57419d3ad2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyzer' is not defined"
     ]
    }
   ],
   "source": [
    "vs = analyzer.polarity_scores(sentence)\n",
    "print (vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Classifier/data_manual_axel_plus_turk.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d142d4a9c8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#with open('../twitter-hatespeech/data/data_manual.json') as json_file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Classifier/data_manual_axel_plus_turk.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Classifier/data_manual_axel_plus_turk.json'"
     ]
    }
   ],
   "source": [
    "json_samples = []\n",
    "\n",
    "#with open('../twitter-hatespeech/data/data_manual.json') as json_file:  \n",
    "with open('../Classifier/data_manual_axel_plus_turk.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i, p in enumerate(data):\n",
    "        text = p['text']\n",
    "        \n",
    "        \n",
    "        if i < 53:\n",
    "            continue\n",
    "            \n",
    "        if i == 1197:\n",
    "            continue\n",
    "        #if i > 10:\n",
    "        #    break\n",
    "        \n",
    "        print(i)\n",
    "        # List to store the sentences and their emotions\n",
    "        sentences = []\n",
    "        \n",
    "        # Split into sentences\n",
    "        tokenized_sentences = nltk.sent_tokenize(text)\n",
    "        \n",
    "        found = False\n",
    "        for sentence in tokenized_sentences:\n",
    "            sentence = sentence.strip()\n",
    "            # Get Vader sentiment\n",
    "            #sentence = sentence.encode('utf-8')\n",
    "            #sentence = bytes(sentence, 'utf-8').decode('utf-8', 'ignore')\n",
    "            print (sentence)\n",
    "            #print()\n",
    "            \n",
    "            res = jammin_emotion(sentence)\n",
    "            \n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            \n",
    "            \n",
    "            # Add the sentence data to the list\n",
    "            sent_data = {\"text\":sentence,\"vader\":vs,\"jammin\":res}\n",
    "            #sent_data = {\"text\":sentence,\"vader\":vs}\n",
    "            sentences.append(sent_data)\n",
    "            #count = count + 1\n",
    "        \n",
    "        #print (sentences)\n",
    "        p[\"sentences\"] = sentences\n",
    "        json_samples.append(p)\n",
    "        \n",
    "        #print('')\n",
    "\n",
    "with open(\"data_manual_axel_plus_turk_sentiment.json\",\"w\") as outfile:  \n",
    "    json.dump(json_samples, outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the emotions to include as part of the binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gross': 0, 'regret': 0, 'numb': 0, 'angry': 0, 'bullying': 0, 'nauseous': 0, 'horrible': 0, 'aggravated': 0, 'awful': 0, 'bored': 0, 'furious': 0, 'bummed': 0, 'aggressiveness': 0, 'disgust': 0, 'annoyed': 0, 'hungover': 0, 'blah': 0, 'annoyance': 0, 'dumb': 0, 'anger': 0, 'mad': 0, 'fedup': 0, 'shame': 0, 'frustrated': 0, 'disgusted': 0, 'terrible': 0, 'rage': 0, 'remorse': 0, 'lousy': 0, 'meh': 0, 'defeated': 0, 'cheated': 0, 'stupid': 0, 'lame': 0, 'pissedoff': 0, 'anger2': 0, 'ugly': 0, 'pissed': 0, 'boredom': 0, 'rough': 0, 'insulted': 0, 'grumpy': 0, 'offended': 0, 'contempt': 0, 'enraged': 0, 'bitter': 0, 'yucky': 0, 'ill': 0, 'irritated': 0}\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "f = open(\"features.txt\", \"r\")\n",
    "features = {}\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    if not \"#\" in line and len(line) > 0:\n",
    "        features[line] = line\n",
    "\n",
    "\n",
    "\n",
    "emotion_count = features\n",
    "for emotion in emotion_count:\n",
    "    emotion_count[emotion] = 0\n",
    "print(emotion_count)\n",
    "print(len(emotion_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gross': 0, 'regret': 0, 'numb': 0, 'angry': 0, 'bullying': 0, 'nauseous': 0, 'horrible': 0, 'aggravated': 0, 'awful': 0, 'bored': 0, 'furious': 0, 'bummed': 0, 'aggressiveness': 0, 'disgust': 0, 'annoyed': 0, 'hungover': 0, 'blah': 0, 'annoyance': 0, 'dumb': 0, 'anger': 0, 'mad': 0, 'fedup': 0, 'shame': 0, 'frustrated': 0, 'disgusted': 0, 'terrible': 0, 'rage': 0, 'remorse': 0, 'lousy': 0, 'meh': 0, 'defeated': 0, 'cheated': 0, 'stupid': 0, 'lame': 0, 'pissedoff': 0, 'anger2': 0, 'ugly': 0, 'pissed': 0, 'boredom': 0, 'rough': 0, 'insulted': 0, 'grumpy': 0, 'offended': 0, 'contempt': 0, 'enraged': 0, 'bitter': 0, 'yucky': 0, 'ill': 0, 'irritated': 0}\n"
     ]
    }
   ],
   "source": [
    "print(emotion_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gross', 'regret', 'numb', 'angry', 'bullying', 'nauseous', 'horrible', 'aggravated', 'awful', 'bored', 'furious', 'bummed', 'aggressiveness', 'disgust', 'annoyed', 'hungover', 'blah', 'annoyance', 'dumb', 'anger', 'mad', 'fedup', 'shame', 'frustrated', 'disgusted', 'terrible', 'rage', 'remorse', 'lousy', 'meh', 'defeated', 'cheated', 'stupid', 'lame', 'pissedoff', 'anger2', 'ugly', 'pissed', 'boredom', 'rough', 'insulted', 'grumpy', 'offended', 'contempt', 'enraged', 'bitter', 'yucky', 'ill', 'irritated'])\n"
     ]
    }
   ],
   "source": [
    "keys = emotion_count.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_to_vector(output, sentiment = True, thresholds = True, normalized=True):\n",
    "    vectors = []\n",
    "    with open(\"data_manual_axel_plus_turk_sentiment.json\") as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        for post in data:\n",
    "            emotions_map = emotion_count.copy()\n",
    "            if sentiment:\n",
    "               \n",
    "                emotions_map[\"neg\"] = 0\n",
    "                emotions_map[\"neu\"] = 0\n",
    "                emotions_map[\"pos\"] = 0\n",
    "                emotions_map[\"compound\"] = 0\n",
    "            num_sentences = len(post[\"sentences\"])\n",
    "            for sentence in post[\"sentences\"]:\n",
    "                if sentiment:\n",
    "                    vader = sentence[\"vader\"]\n",
    "                    #print (vader)\n",
    "                    emotions_map[\"neg\"] += vader[\"neg\"]\n",
    "                    emotions_map[\"neu\"] += vader[\"neu\"]\n",
    "                    emotions_map[\"pos\"] += vader[\"pos\"]\n",
    "                    emotions_map[\"compound\"] += vader[\"compound\"] \n",
    "                    \n",
    "                groups = sentence[\"jammin\"][\"groups\"]\n",
    "                for group in groups:\n",
    "                    if \"anger\" in group[\"name\"] or \"disgust\" in group[\"name\"]:\n",
    "                        for emotion in group[\"emotions\"]:\n",
    "                            if emotion in emotions_map:\n",
    "                                emotions_map[emotion] += 1.0\n",
    "\n",
    "            vector = []\n",
    "            if normalized:\n",
    "                for count in emotions_map.values():\n",
    "                    vector.append(count/float(num_sentences))\n",
    "            else:\n",
    "                for count in emotions_map.values():\n",
    "                    vector.append(count)\n",
    "                    \n",
    "            if thresholds:\n",
    "                vector = vector + get_negativity_ind_posts(post,0,  0, 0)\n",
    "\n",
    "            vectors.append(vector)\n",
    "    with open(output,\"w\") as outfile:  \n",
    "        json.dump(vectors, outfile)\n",
    "        outfile.close()\n",
    "    print(emotions_map)\n",
    "    return vectors\n",
    "\n",
    "def some_data_to_vector(indices, output, sentiment = True, normalized=True):\n",
    "    vectors = []\n",
    "    with open(\"data_manual_sentiment.json\") as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        for index in indices:\n",
    "            post = data[index]\n",
    "            emotions_map = emotion_count.copy()\n",
    "            if sentiment:\n",
    "               \n",
    "                emotions_map[\"neg\"] = 0\n",
    "                emotions_map[\"neu\"] = 0\n",
    "                emotions_map[\"pos\"] = 0\n",
    "                emotions_map[\"compound\"] = 0\n",
    "            num_sentences = len(post[\"sentences\"])\n",
    "            for sentence in post[\"sentences\"]:\n",
    "                if sentiment:\n",
    "                    vader = sentence[\"vader\"]\n",
    "                    #print (vader)\n",
    "                    emotions_map[\"neg\"] += vader[\"neg\"]\n",
    "                    emotions_map[\"neu\"] += vader[\"neu\"]\n",
    "                    emotions_map[\"pos\"] += vader[\"pos\"]\n",
    "                    emotions_map[\"compound\"] += vader[\"compound\"] \n",
    "                    \n",
    "                groups = sentence[\"jammin\"][\"groups\"]\n",
    "                for group in groups:\n",
    "                    if \"anger\" in group[\"name\"] or \"disgust\" in group[\"name\"]:\n",
    "                        for emotion in group[\"emotions\"]:\n",
    "                            if emotion in emotions_map:\n",
    "                                emotions_map[emotion] += 1.0\n",
    "\n",
    "            vector = []\n",
    "            for count in emotions_map.values():\n",
    "                vector.append(count/float(num_sentences))\n",
    "\n",
    "            vectors.append(vector)\n",
    "    with open(output,\"w\") as outfile:  \n",
    "        json.dump(vectors, outfile)\n",
    "        outfile.close()\n",
    "    print(emotions_map)\n",
    "    return vectors\n",
    "\n",
    "'''def all_data_to_vector2(output, sentiment=True, emotions=[\"anger\",\"disgust\"], normalized=True):\n",
    "    vectors = []\n",
    "    with open(\"data_manual_sentiment.json\") as json_file:  \n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        for post in data:\n",
    "            emotions_map = emotion_count.copy()\n",
    "            if sentiment:\n",
    "                print (\"Including sentiment\")\n",
    "                emotions_map[\"neg\"] = 0\n",
    "                emotions_map[\"neu\"] = 0\n",
    "                emotions_map[\"pos\"] = 0\n",
    "                emotions_map[\"compound\"] = 0\n",
    "            \n",
    "            num_sentences = len(post[\"sentences\"])\n",
    "            for sentence in post[\"sentences\"]:\n",
    "                # Sentiment section\n",
    "                if sentiment:\n",
    "                    vader = sentence[\"vader\"]\n",
    "                    emotions_map[\"neg\"] += vader[\"neg\"]\n",
    "                    emotions_map[\"neu\"] += vader[\"neu\"]\n",
    "                    emotions_map[\"pos\"] += vader[\"pos\"]\n",
    "                    emotions_map[\"compound\"] += vader[\"compound\"]\n",
    "                    \n",
    "                \n",
    "                # Emotion section\n",
    "                groups = sentence[\"jammin\"][\"groups\"]\n",
    "                # For each emotion group\n",
    "                for group in groups:\n",
    "                    # For each emotion to include\n",
    "                    #for emo in emotions:\n",
    "                        # If the group name matches an emotion to include\n",
    "                        if \"anger\" in group[\"name\"]:\n",
    "                            # If a subemotion within the group is in the emotion map\n",
    "                            for emotion in group[\"emotions\"]:\n",
    "                                if emotion in emotions_map:\n",
    "                                    emotions_map[emotion] += 1.0\n",
    "\n",
    "            for count in emotions_map.values():\n",
    "                vector.append(count/num_sentences)\n",
    "\n",
    "            vectors.append(vector)\n",
    "            print (vector)\n",
    "    with open(output,\"w\") as outfile:  \n",
    "        json.dump(vectors, outfile)\n",
    "        outfile.close()\n",
    "    return vectors\n",
    "'''\n",
    "\n",
    "def single_text_to_vector(text, sentiment = True, normalized=True, thresholds=True):\n",
    "    emotions_map = emotion_count.copy()\n",
    "    if sentiment:\n",
    "        emotions_map[\"neg\"] = 0\n",
    "        emotions_map[\"neu\"] = 0\n",
    "        emotions_map[\"pos\"] = 0\n",
    "        emotions_map[\"compound\"] = 0\n",
    "            \n",
    "    # Split into sentences\n",
    "    tokenized_sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = len(tokenized_sentences)\n",
    "    for sentence in tokenized_sentences:\n",
    "        if sentiment:\n",
    "            vader = analyzer.polarity_scores(sentence)\n",
    "            print (vader)\n",
    "            emotions_map[\"neg\"] += vader[\"neg\"]\n",
    "            emotions_map[\"neu\"] += vader[\"neu\"]\n",
    "            emotions_map[\"pos\"] += vader[\"pos\"]\n",
    "            emotions_map[\"compound\"] += vader[\"compound\"] \n",
    "                \n",
    "        res = jammin_emotion(sentence)\n",
    "        print (res)\n",
    "        print()\n",
    "        groups = res[\"groups\"]\n",
    "        for group in groups:\n",
    "            if \"anger\" in group[\"name\"] or \"disgust\" in group[\"name\"]:\n",
    "                for emotion in group[\"emotions\"]:\n",
    "                    if emotion in emotions_map:\n",
    "                        emotions_map[emotion] += 1.0\n",
    "\n",
    "    vector = []\n",
    "    if normalized:\n",
    "        for count in emotions_map.values():\n",
    "            vector.append(count/float(num_sentences))\n",
    "    else:\n",
    "        for count in emotions_map.values():\n",
    "            vector.append(count)\n",
    "                    \n",
    "    #if thresholds:\n",
    "        #vector = vector + get_negativity_ind_posts(text,0,  0, 0)\n",
    "            \n",
    "    return [vector]\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "                            \n",
    "        \n",
    "                    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c8cb428743ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"features_vader_jammin(disgust_anger)_norm.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#input_file = \"data_manual_sentiment.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data_to_vector\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f5f4522de81d>\u001b[0m in \u001b[0;36mall_data_to_vector\u001b[0;34m(output, sentiment, thresholds, normalized)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_manual_axel_plus_turk_sentiment.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "output = \"features_vader_jammin(disgust_anger)_norm.json\"\n",
    "#input_file = \"data_manual_sentiment.json\"\n",
    "vectors = all_data_to_vector( output, normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3bfa0da7d316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vectors[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-da88b50631cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_text_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Obama is a Muslim and was born in Africa. His whole mission is to start the downfall of America by allowing the invasion of illegal immigrants and Muslim terrorist!\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-f5f4522de81d>\u001b[0m in \u001b[0;36msingle_text_to_vector\u001b[0;34m(text, sentiment, normalized, thresholds)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Split into sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mnum_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "vectorx = single_text_to_vector(\"Obama is a Muslim and was born in Africa. His whole mission is to start the downfall of America by allowing the invasion of illegal immigrants and Muslim terrorist!\",sentiment = True, normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6e2380c77ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorx' is not defined"
     ]
    }
   ],
   "source": [
    " print(vectorx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-56a47ee1d6d9>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-56a47ee1d6d9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    0: \"horrible\" 1\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0: \"horrible\" 1\n",
    "1: \"ugly\" 1\n",
    "2: \"disgusted\" 2 \n",
    "    \n",
    "0: \"meh\" 1\n",
    "1: \"blah\" 1 \n",
    "2: \"bitter\" 1\n",
    "    \n",
    "\n",
    "1: \"defeated\" 1\n",
    "2: \"dumb\" 1\n",
    "3: \"terrible\" 1\n",
    "    \n",
    "0: \"grumpy\" 1\n",
    "1: \"angry\" 1\n",
    "2: \"fedup\" 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-89fbab84dda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_manual_axel_plus_turk_sentiment.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "indices_1 = []\n",
    "indices_0 = []\n",
    "\n",
    "with open(\"data_manual_axel_plus_turk_sentiment.json\") as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    X = []\n",
    "    y = []\n",
    "    for index, post in enumerate(data):\n",
    "        text = post['text']\n",
    "        X.append(text)\n",
    "        \n",
    "        if \"no_hs\" in post[\"label\"]:\n",
    "            #if len(indices_0) < 330:\n",
    "            y.append(0)\n",
    "            indices_0.append(index)\n",
    "        else:\n",
    "            y.append(1)\n",
    "            indices_1.append(index)\n",
    "print(y.count(0))\n",
    "print(y.count(1))\n",
    "print(len(indices_0))\n",
    "print(len(indices_1))\n",
    "#indices = indices_0 + indices_1\n",
    "#output = \"./some_features_vader_jammin(disgust_anger)_norm.json\"\n",
    "#vectors = some_data_to_vector(indices, output)\n",
    "\n",
    "vectors = all_data_to_vector( output, sentiment = True, thresholds = False, normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c47616a68fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-82998f1237a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m            'f1_micro','f1_macro','f1_weighted']\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#c = svm.SVC(C=1, kernel='linear')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "#X = [[0], [1], [2], [3]]\n",
    "#Y = [0, 1, 2, 3]\n",
    "scoring = ['accuracy','precision', 'recall','f1',\n",
    "           'precision_micro', 'precision_macro', 'precision_weighted',\n",
    "           'recall_micro', 'recall_macro', 'recall_weighted',\n",
    "           'f1_micro','f1_macro','f1_weighted']\n",
    "\n",
    "c = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "#c = svm.SVC(C=1, kernel='linear')\n",
    "\n",
    "Cs = np.logspace(-6, 3, 10)\n",
    "grid={\"C\":Cs, \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "#grid={\"C\":Cs}\n",
    "c=LogisticRegression()\n",
    "clf = GridSearchCV(estimator=c, param_grid=grid,n_jobs=-1)\n",
    "clf.fit(vectors, y) \n",
    "\n",
    "normalized_X = preprocessing.normalize(vectors)\n",
    "scores = cross_validate(clf, normalized_X, y, cv=10, scoring=scoring)\n",
    "print(scores)\n",
    "print(clf.best_score_)                              \n",
    "print(clf.best_estimator_.C)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'printScores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a01e9d28cfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprintScores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'printScores' is not defined"
     ]
    }
   ],
   "source": [
    "printScores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-76f88bf60a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_text_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are such a piece of shit. I will beat the crap out of you. You have such a horrible face. Eat shit bitch!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f5f4522de81d>\u001b[0m in \u001b[0;36msingle_text_to_vector\u001b[0;34m(text, sentiment, normalized, thresholds)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# Split into sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mtokenized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mnum_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "vector = single_text_to_vector(\"You are such a piece of shit. I will beat the crap out of you. You have such a horrible face. Eat shit bitch!\")\n",
    "print (vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0b56ada90d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.predict(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro\n",
      "average precision is 0.538846\n",
      "average recall is 0.534350\n",
      "average f1 is 0.527453\n",
      "Micro\n",
      "average precision is 0.534638\n",
      "average recall is 0.534638\n",
      "average f1 is 0.534638\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "test_precision_micro = mean([0.48148148, 0.48148148, 0.51851852, 0.54074074, 0.56296296,\n",
    "       0.49253731, 0.56716418, 0.52238806, 0.59701493, 0.58208955])\n",
    "\n",
    "test_precision_macro = mean([0.48070919, 0.48094816, 0.51837588, 0.54644525, 0.56333333,\n",
    "       0.49247698, 0.56740443, 0.52457814, 0.61754386, 0.59664306])\n",
    "\n",
    "test_recall_micro = mean([0.48148148, 0.48148148, 0.51851852, 0.54074074, 0.56296296,\n",
    "       0.49253731, 0.56716418, 0.52238806, 0.59701493, 0.58208955])\n",
    "\n",
    "test_recall_macro = mean([0.48101405, 0.48112379, 0.51832748, 0.53928885, 0.56255487,\n",
    "       0.49253731, 0.56716418, 0.52238806, 0.59701493, 0.58208955])\n",
    "    \n",
    "test_f1_micro = mean([0.48148148, 0.48148148, 0.51851852, 0.54074074, 0.56296296,\n",
    "       0.49253731, 0.56716418, 0.52238806, 0.59701493, 0.58208955])\n",
    "    \n",
    "test_f1_macro = mean([0.47916667, 0.48008363, 0.51809545, 0.52160494, 0.56142283,\n",
    "       0.49151786, 0.56677815, 0.51150604, 0.57861635, 0.56574074])\n",
    "\n",
    "print(\"Macro\")\n",
    "print(\"average precision is %f\"%test_precision_macro)\n",
    "print(\"average recall is %f\"%test_recall_macro)\n",
    "print(\"average f1 is %f\"%test_f1_macro)\n",
    "\n",
    "print(\"Micro\")\n",
    "print(\"average precision is %f\"%test_precision_micro)\n",
    "print(\"average recall is %f\"%test_recall_micro)\n",
    "print(\"average f1 is %f\"%test_f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for turkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3bd3adcab978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_manual_sentiment.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"turk.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"data_manual_sentiment.json\") as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    count = 0\n",
    "    \n",
    "    out = open(\"turk.csv\",\"w\")\n",
    "    out.write(\"source,sample\\n\")\n",
    "    for index, post in enumerate(data):\n",
    "        if not \"no_hs\" in post[\"label\"]:\n",
    "            \n",
    "            if len(post[\"text\"]) <= 1000:\n",
    "                count += 1\n",
    "                text = post[\"text\"] \n",
    "                text = text.replace(\",\",\" \")\n",
    "                # Pre process text\n",
    "                text = text.replace('\"','\\\\\"')\n",
    "                text = text.replace(\"\\n\",\" \")\n",
    "                text = text.replace('â€™',\"'\")\n",
    "                text = text.replace('â€œ','\\\\\"')\n",
    "                text = text.replace('â€','\\\\\"')\n",
    "                text = text.replace('â€”','-')\n",
    "                text = text.replace('â€¦','...')\n",
    "                text = text.replace('ðŸ¤”',' ')\n",
    "                text = text.replace('Ÿ™„',' ')\n",
    "                text = text.replace('â€˜',\"'\")\n",
    "                text = text.replace('ðŸ‘',' ')\n",
    "                text = text.replace('ðŸ˜‰',' ')\n",
    "                text = text.replace('ðŸ¤¨',' ')\n",
    "                text = text.replace('ðŸ˜„',' ')\n",
    "                text = text.replace('Ÿ˜‚',' ')\n",
    "                text = text.replace('Ÿ˜',' ')\n",
    "                text = text.replace('€‹',' ')\n",
    "                text = text.replace('€‹',' ')\n",
    "                text = text.replace('„',' ')\n",
    "                text = text.replace('Ÿ',' ')\n",
    "                text = text.replace('€',' ')\n",
    "                text = text.replace('™‚',' ')\n",
    "                text = text.replace('’',' ')\n",
    "                text = text.replace('™',' ')\n",
    "                text = text.replace('‡',' ')\n",
    "                text = text.replace('‘Š',' ')\n",
    "                text = text.replace('–',' ')\n",
    "                text = text.replace('“',' ')\n",
    "                text = text.replace('”',' ')\n",
    "                text = text.replace('š',' ')\n",
    "                text = text.replace('˜',' ')\n",
    "                text = text.replace('œ…',' ')\n",
    "                text = text.replace('œ',' ')\n",
    "                text = text.replace('‹',' ')\n",
    "                text = text.replace('Œ',' ')\n",
    "                text = text.replace('›',' ')\n",
    "                text = text.replace('‘',' ')\n",
    "                text = text.replace('…',' ')\n",
    "                text = text.replace('Ž',' ')\n",
    "                #print(post[\"text\"])\n",
    "            #else: \n",
    "                #print(\"*\"*10+str(index)+\"*\"*10)\n",
    "                #print(len(post[\"text\"]))\n",
    "                out.write(str(index)+ \", \\\"\"+ text+\"\\\"\\n\")\n",
    "            \n",
    "    out.close()\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Additions - After checking paper 2 and noticying it killed us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7fd15dc02e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_manual_axel_plus_turk_sentiment.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "indices_1 = []\n",
    "indices_0 = []\n",
    "\n",
    "with open(\"data_manual_axel_plus_turk_sentiment.json\") as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    X = []\n",
    "    y = []\n",
    "    for index, post in enumerate(data):\n",
    "        text = post['text']\n",
    "        X.append(text)\n",
    "        \n",
    "        if \"no_hs\" in post[\"label\"]:\n",
    "            #if len(indices_0) < 330:\n",
    "            y.append(0)\n",
    "            indices_0.append(index)\n",
    "        else:\n",
    "            y.append(1)\n",
    "            indices_1.append(index)\n",
    "print(y.count(0))\n",
    "print(y.count(1))\n",
    "print(len(indices_0))\n",
    "print(len(indices_1))\n",
    "#indices = indices_0 + indices_1\n",
    "#output = \"./some_features_vader_jammin(disgust_anger)_norm.json\"\n",
    "#vectors = some_data_to_vector(indices, output)\n",
    "\n",
    "vectors = all_data_to_vector( output, sentiment = True, thresholds = False, normalized=False)\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-dd6f2dc67eea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range = (1, 1))\n",
    "X_train_counts = count_vect.fit_transform(X)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "#X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf = X_train_counts\n",
    "print(X_train_tf.shape)\n",
    "\n",
    "X=X_train_tf.tocoo()\n",
    "\n",
    "X = X.todense().tolist()\n",
    "\n",
    "for i,row in enumerate(X):\n",
    "    vectors[i] = vectors[i] + row\n",
    "\n",
    "print(len(vectors[9]))\n",
    "\n",
    "vectors = scipy.sparse.csr_matrix(vectors)\n",
    "print(type(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-52b1ef3ab04e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tf' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(X_train_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7c47bf9756d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tf' is not defined"
     ]
    }
   ],
   "source": [
    "X=X_train_tf.tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-84f532ce6ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4459a5b16894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e1884873e19e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-dab4864b2c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X = X.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-fa0134f942c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-64cfd55e9633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1318\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X[0][1318])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a2c98f55aea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "for i,row in enumerate(X):\n",
    "    vectors[i] = vectors[i] + row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c3afcdda5ff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(vectors[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-12f3da266aba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "vectors = scipy.sparse.csr_matrix(vectors)\n",
    "print(type(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGDClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a092685703ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#c = svm.SVC(kernel='linear', C=1, random_state=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#c = svm.SVC(C=1, kernel='linear')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m c = SGDClassifier(loss='hinge', penalty='l2', # hinge gives an SVM\n\u001b[0m\u001b[1;32m      9\u001b[0m                                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                            max_iter=5, tol=None)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGDClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "scoring = ['accuracy','precision', 'recall','f1',\n",
    "           'precision_micro', 'precision_macro', 'precision_weighted',\n",
    "           'recall_micro', 'recall_macro', 'recall_weighted',\n",
    "           'f1_micro','f1_macro','f1_weighted']\n",
    "\n",
    "#c = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "#c = svm.SVC(C=1, kernel='linear')\n",
    "c = SGDClassifier(loss='hinge', penalty='l2', # hinge gives an SVM\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=5, tol=None)\n",
    "Cs = np.logspace(-6, 3, 10)\n",
    "#grid={\"C\":Cs, \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "grid={\"alpha\":(1e-1, 1e-2, 1e-3, 1e-4), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "#grid={\"C\":Cs}\n",
    "#c=LogisticRegression()\n",
    "\n",
    "vectors = preprocessing.normalize(vectors)\n",
    "clf = GridSearchCV(estimator=c, param_grid=grid,n_jobs=-1)\n",
    "clf.fit(vectors, y) \n",
    "\n",
    "\n",
    "scores = cross_validate(clf, vectors, y, cv=10, scoring=scoring)\n",
    "print(scores)\n",
    "print(clf.best_score_)                              \n",
    "#print(clf.best_estimator_.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-779c57b65674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(grid.keys()):\n",
    "    print(\"%s: %r\" % (param_name, clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0fc70a5bb236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average recall is %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtest_recall_micro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average f1 is %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtest_f1_micro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprintScores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "def printScores(scores):\n",
    "    from statistics import mean\n",
    "    test_precision_micro = mean(scores['test_precision_micro'])\n",
    "\n",
    "    test_precision_macro = mean(scores['test_precision_macro'])\n",
    "\n",
    "    test_recall_micro = mean(scores['test_recall_micro'])\n",
    "\n",
    "    test_recall_macro = mean(scores['test_recall_macro'])\n",
    "\n",
    "    test_f1_micro = mean(scores['test_f1_micro'])\n",
    "\n",
    "    test_f1_macro = mean(scores['test_f1_macro'])\n",
    "\n",
    "    print(\"Macro\")\n",
    "    print(\"average precision is %f\"%test_precision_macro)\n",
    "    print(\"average recall is %f\"%test_recall_macro)\n",
    "    print(\"average f1 is %f\"%test_f1_macro)\n",
    "\n",
    "    print(\"Micro\")\n",
    "    print(\"average precision is %f\"%test_precision_micro)\n",
    "    print(\"average recall is %f\"%test_recall_micro)\n",
    "    print(\"average f1 is %f\"%test_f1_micro)\n",
    "printScores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7540bcc47c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mjammin_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I am sorry that I was too busy to meet, but I wish you all the best today!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-7540bcc47c9b>\u001b[0m in \u001b[0;36mjammin_emotion\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#url = 'http://127.0.0.1:8080/JamminTextEmotionAPI/webresources/jammin/emotion'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print(r)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "def jammin_emotion(text):\n",
    "   \n",
    "    payload = '{\"text\":\"%s\",\"lang\":\"en\"}'%(text)\n",
    "    #print payload\n",
    "    headers = {'content-Type': 'application/json'}\n",
    "\n",
    "    url = 'http://1.34.96.63:8080/webresources/jammin/emotion'\n",
    "    #url = 'http://127.0.0.1:8080/JamminTextEmotionAPI/webresources/jammin/emotion'\n",
    "\n",
    "    r = requests.post(url,  data=payload, headers=headers)\n",
    "    #print(r)\n",
    "    return r.json()\n",
    "\n",
    "jammin_emotion(\"I am sorry that I was too busy to meet, but I wish you all the best today!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
